{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "combined_notebook"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import traceback\n",
        "from datetime import datetime\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, HashingTF, StringIndexer\n",
        "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, NaiveBayes\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "import re\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spark_setup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "448a0fc7-35f5-4676-b68b-55347c929f66"
      },
      "source": [
        "\n",
        "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
        "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
        "\n",
        "winutils_path = \"C:\\\\hadoop\"\n",
        "if os.path.exists(winutils_path):\n",
        "    os.environ[\"HADOOP_HOME\"] = winutils_path\n",
        "    os.environ[\"PATH\"] = f\"{os.environ['PATH']};{winutils_path}\\\\bin\"\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Amazon Reviews Sentiment Analysis\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "    .config(\"spark.python.worker.memory\", \"512m\") \\\n",
        "    .config(\"spark.python.worker.timeout\", \"600\") \\\n",
        "    .config(\"spark.network.timeout\", \"600s\") \\\n",
        "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
        "    .master(\"local[2]\") \\\n",
        "    .getOrCreate()\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "print(\"Spark session initialized successfully\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark session initialized successfully\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "load_and_clean_data"
      },
      "source": [
        "def load_and_clean_data(spark, data_path=\"Data.json\"):\n",
        "    print(f\"Loading data from {data_path}...\")\n",
        "    try:\n",
        "        # Load JSON data\n",
        "        reviews_df = spark.read.json(data_path)\n",
        "        print(f\"Number of raw records: {reviews_df.count()}\")\n",
        "\n",
        "        # Create sentiment label based on overall rating\n",
        "        reviews_df = reviews_df.withColumn(\n",
        "            \"sentiment\",\n",
        "            when(col(\"overall\") < 3, 0)  # Negative\n",
        "            .when(col(\"overall\") == 3, 1)  # Neutral\n",
        "            .otherwise(2)  # Positive\n",
        "        )\n",
        "\n",
        "        # Basic data cleaning\n",
        "        reviews_df = reviews_df.filter(col(\"reviewText\").isNotNull())\n",
        "        reviews_df = reviews_df.withColumn(\"cleaned_reviews\", regexp_replace(col(\"reviewText\"), \"[^a-zA-Z\\\\s]\", \" \"))\n",
        "        reviews_df = reviews_df.withColumn(\"cleaned_reviews\", lower(col(\"cleaned_reviews\")))\n",
        "        reviews_df = reviews_df.withColumn(\"cleaned_reviews\", trim(col(\"cleaned_reviews\")))\n",
        "\n",
        "        # Convert sentiment to textual labels for consistency with best_model_training\n",
        "        reviews_df = reviews_df.withColumn(\n",
        "            \"label\",\n",
        "            when(col(\"sentiment\") == 0, \"negative\")\n",
        "            .when(col(\"sentiment\") == 1, \"neutral\")\n",
        "            .otherwise(\"positive\")\n",
        "        )\n",
        "\n",
        "        # Select relevant columns\n",
        "        df = reviews_df.select(\"cleaned_reviews\", \"label\").cache()\n",
        "        print(f\"Number of valid records after cleaning: {df.count()}\")\n",
        "        df.show(3, truncate=50)\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading or cleaning data: {str(e)}\")\n",
        "        traceback.print_exc()\n",
        "        return None"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "balance_data"
      },
      "source": [
        "from pyspark.sql.functions import col, broadcast\n",
        "import builtins\n",
        "\n",
        "def balance_data_alt(df):\n",
        "    print(\"Balancing data using oversampling...\")\n",
        "\n",
        "    class_counts = df.groupBy(\"label\").count()\n",
        "    class_counts.show()\n",
        "\n",
        "    counts_dict = {row[\"label\"]: row[\"count\"] for row in class_counts.collect()}\n",
        "\n",
        "    max_count = builtins.max(counts_dict.values())\n",
        "    print(f\"Maximum class count: {max_count}\")\n",
        "\n",
        "    balanced_dfs = []\n",
        "\n",
        "    for label, count in counts_dict.items():\n",
        "        if count < max_count:\n",
        "            # Filter for this class\n",
        "            class_df = df.filter(col(\"label\") == label)\n",
        "\n",
        "            # Calculate sampling fraction (with replacement)\n",
        "            sampling_fraction = max_count / count\n",
        "            print(f\"Oversampling class '{label}' with fraction {sampling_fraction}\")\n",
        "\n",
        "            # Sample with replacement\n",
        "            oversampled_df = class_df.sample(\n",
        "                withReplacement=True,\n",
        "                fraction=sampling_fraction,\n",
        "                seed=42\n",
        "            )\n",
        "\n",
        "            # Ensure exactly max_count rows\n",
        "            if oversampled_df.count() != max_count:\n",
        "                print(f\"Adjusting sample size for class '{label}'\")\n",
        "                oversampled_df = oversampled_df.limit(max_count)\n",
        "\n",
        "            balanced_dfs.append(oversampled_df)\n",
        "        else:\n",
        "            print(f\"Keeping class '{label}' as is (majority class)\")\n",
        "            balanced_dfs.append(df.filter(col(\"label\") == label))\n",
        "\n",
        "    # Union all balanced dataframes\n",
        "    balanced_df = balanced_dfs[0]\n",
        "    for i in range(1, len(balanced_dfs)):\n",
        "        balanced_df = balanced_df.union(balanced_dfs[i])\n",
        "\n",
        "    # Verify final distribution\n",
        "    print(\"Class distribution after balancing:\")\n",
        "    balanced_df.groupBy(\"label\").count().show()\n",
        "\n",
        "    return balanced_df"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "split_data"
      },
      "source": [
        "def split_data(df):\n",
        "    print(\"Splitting data...\")\n",
        "    train_df, temp_df = df.randomSplit([0.8, 0.2], seed=42)\n",
        "    val_df, test_df = temp_df.randomSplit([0.5, 0.5], seed=42)\n",
        "    print(f\"Training data: {train_df.count()} examples\")\n",
        "    print(f\"Validation data: {val_df.count()} examples\")\n",
        "    print(f\"Test data: {test_df.count()} examples\")\n",
        "    return train_df, val_df, test_df"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "save_model_and_data"
      },
      "source": [
        "def save_model_locally(model, model_name, output_dir=\"models\"):\n",
        "    try:\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        model_path = os.path.join(output_dir, f\"{model_name.lower().replace(' ', '_')}_model\")\n",
        "        print(f\"Saving model to: {model_path}\")\n",
        "        model.write().overwrite().save(model_path)\n",
        "        print(f\"Model saved successfully: {model_path}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving model: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "def save_test_data_locally(test_data, output_file=\"data/processed/test_data.json\"):\n",
        "    try:\n",
        "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "        test_data_pandas = test_data.toPandas()\n",
        "        test_data_pandas.to_json(output_file, orient=\"records\", force_ascii=False, indent=4)\n",
        "        print(f\"Test data saved to: {output_file}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving test data: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return False"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "train_and_evaluate_models"
      },
      "source": [
        "def train_and_evaluate_models(train_df, val_df, test_df):\n",
        "    print(\"Configuring pipelines...\")\n",
        "\n",
        "    # Create pipeline stages\n",
        "    tokenizer = Tokenizer(inputCol=\"cleaned_reviews\", outputCol=\"words\")\n",
        "    remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
        "    cv = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"raw_features\", minDF=2.0)\n",
        "    idf = IDF(inputCol=\"raw_features\", outputCol=\"features\", minDocFreq=2)\n",
        "    hashing_tf = HashingTF(inputCol=\"filtered_words\", outputCol=\"hashing_features\", numFeatures=5000)\n",
        "    label_indexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(train_df)\n",
        "\n",
        "    # Define models with optimized parameters\n",
        "    lr = LogisticRegression(\n",
        "        featuresCol=\"features\",\n",
        "        labelCol=\"indexedLabel\",\n",
        "        maxIter=100,\n",
        "        regParam=1/6866.498,  # From best_model_training\n",
        "        elasticNetParam=0.0,\n",
        "        family=\"multinomial\"\n",
        "    )\n",
        "    rf = RandomForestClassifier(\n",
        "        featuresCol=\"features\",\n",
        "        labelCol=\"indexedLabel\",\n",
        "        numTrees=100,\n",
        "        maxDepth=10,\n",
        "        seed=42\n",
        "    )\n",
        "    nb = NaiveBayes(\n",
        "        featuresCol=\"features\",\n",
        "        labelCol=\"indexedLabel\",\n",
        "        smoothing=1.0,\n",
        "        modelType=\"multinomial\"\n",
        "    )\n",
        "\n",
        "    # Create pipelines\n",
        "    lr_pipeline = Pipeline(stages=[tokenizer, remover, cv, idf, label_indexer, lr])\n",
        "    rf_pipeline = Pipeline(stages=[tokenizer, remover, cv, idf, label_indexer, rf])\n",
        "    nb_pipeline = Pipeline(stages=[tokenizer, remover, cv, idf, label_indexer, nb])\n",
        "\n",
        "    # Define parameter grids for hyperparameter tuning\n",
        "    lr_param_grid = ParamGridBuilder() \\\n",
        "        .addGrid(lr.regParam, [0.0001, 0.001, 0.01]) \\\n",
        "        .addGrid(lr.elasticNetParam, [0.0, 0.5]) \\\n",
        "        .build()\n",
        "    rf_param_grid = ParamGridBuilder() \\\n",
        "        .addGrid(rf.numTrees, [50, 100]) \\\n",
        "        .addGrid(rf.maxDepth, [5, 10]) \\\n",
        "        .build()\n",
        "    nb_param_grid = ParamGridBuilder() \\\n",
        "        .addGrid(nb.smoothing, [0.5, 1.0, 2.0]) \\\n",
        "        .build()\n",
        "\n",
        "    # Evaluator\n",
        "    evaluator = MulticlassClassificationEvaluator(\n",
        "        labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
        "    )\n",
        "\n",
        "    # Cross-validation for each model\n",
        "    models = [\n",
        "        (\"Logistic Regression\", lr_pipeline, lr_param_grid),\n",
        "        (\"Random Forest\", rf_pipeline, rf_param_grid),\n",
        "        (\"Naive Bayes\", nb_pipeline, nb_param_grid)\n",
        "    ]\n",
        "\n",
        "    best_model = None\n",
        "    best_model_name = None\n",
        "    best_val_accuracy = float('-inf')\n",
        "    best_metrics = {}\n",
        "\n",
        "    for name, pipeline, param_grid in models:\n",
        "        print(f\"Training {name} with cross-validation...\")\n",
        "        start_time = datetime.now()\n",
        "        cv = CrossValidator(\n",
        "            estimator=pipeline,\n",
        "            estimatorParamMaps=param_grid,\n",
        "            evaluator=evaluator,\n",
        "            numFolds=3,\n",
        "            seed=42\n",
        "        )\n",
        "        model = cv.fit(train_df)\n",
        "        print(f\"Training time for {name}: {datetime.now() - start_time}\")\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        val_predictions = model.transform(val_df)\n",
        "        val_accuracy = evaluator.evaluate(val_predictions)\n",
        "        print(f\"Validation accuracy for {name}: {val_accuracy:.4f}\")\n",
        "\n",
        "        # Track best model\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            best_model = model\n",
        "            best_model_name = name\n",
        "            best_metrics = {\n",
        "                \"accuracy\": val_accuracy,\n",
        "                \"f1\": evaluator.evaluate(val_predictions, {evaluator.metricName: \"f1\"}),\n",
        "                \"weightedPrecision\": evaluator.evaluate(val_predictions, {evaluator.metricName: \"weightedPrecision\"}),\n",
        "                \"weightedRecall\": evaluator.evaluate(val_predictions, {evaluator.metricName: \"weightedRecall\"})\n",
        "            }\n",
        "\n",
        "        # Show class distribution\n",
        "        print(f\"\\nClass distribution for {name}:\")\n",
        "        val_predictions.groupBy(\"indexedLabel\").count().show()\n",
        "\n",
        "    print(f\"\\nBest model: {best_model_name} with validation accuracy: {best_val_accuracy:.4f}\")\n",
        "    print(\"Metrics for best model:\")\n",
        "    for metric, value in best_metrics.items():\n",
        "        print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "    # Evaluate best model on test set\n",
        "    test_predictions = best_model.transform(test_df)\n",
        "    test_accuracy = evaluator.evaluate(test_predictions)\n",
        "    print(f\"Test accuracy for {best_model_name}: {test_accuracy:.4f}\")\n",
        "\n",
        "    # Save the best model and test data\n",
        "    save_model_locally(best_model, best_model_name)\n",
        "    save_test_data_locally(test_df)\n",
        "\n",
        "    # Save TF-IDF model separately\n",
        "    tfidf_stages = best_model.bestModel.stages[0:4]  # Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
        "    tfidf_pipeline = Pipeline(stages=tfidf_stages)\n",
        "    tfidf_model = tfidf_pipeline.fit(train_df)\n",
        "    save_model_locally(tfidf_model, \"TFIDF\")\n",
        "\n",
        "    return best_model, best_model_name"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "main_function",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d507327-999c-49bb-d87d-33b63d9605e1"
      },
      "source": [
        "def main():\n",
        "    start_time = datetime.now()\n",
        "    print(f\"Starting process at {start_time.strftime('%H:%M:%S')}\")\n",
        "\n",
        "    try:\n",
        "        # Load and clean data\n",
        "        df = load_and_clean_data(spark)\n",
        "        if df is None:\n",
        "            print(\"ERROR: Data not loaded\")\n",
        "            return\n",
        "\n",
        "        # Balance data\n",
        "        balanced_df = balance_data_alt(df)\n",
        "\n",
        "        # Show statistics\n",
        "        print(\"\\nStatistics on balanced data:\")\n",
        "        balanced_df.describe().show()\n",
        "\n",
        "        # Split data\n",
        "        train_df, val_df, test_df = split_data(balanced_df)\n",
        "\n",
        "        # Train and evaluate models\n",
        "        best_model, best_model_name = train_and_evaluate_models(train_df, val_df, test_df)\n",
        "\n",
        "        # Save processed data\n",
        "        train_df.write.mode(\"overwrite\").parquet(\"data/processed/train_data.parquet\")\n",
        "        val_df.write.mode(\"overwrite\").parquet(\"data/processed/val_data.parquet\")\n",
        "        test_df.write.mode(\"overwrite\").parquet(\"data/processed/test_data.parquet\")\n",
        "\n",
        "        print(f\"\\n=== Training completed successfully with best model: {best_model_name} ===\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: {str(e)}\")\n",
        "        traceback.print_exc()\n",
        "    finally:\n",
        "        print(\"\\nCleaning up resources...\")\n",
        "        spark.stop()\n",
        "        print(\"Spark session closed\")\n",
        "        print(f\"Total execution time: {datetime.now() - start_time}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting process at 19:26:31\n",
            "Loading data from Data.json...\n",
            "Number of raw records: 10261\n",
            "Number of valid records after cleaning: 10261\n",
            "+--------------------------------------------------+--------+\n",
            "|                                   cleaned_reviews|   label|\n",
            "+--------------------------------------------------+--------+\n",
            "|not much to write about here  but it does exact...|positive|\n",
            "|the product does exactly as it should and is qu...|positive|\n",
            "|the primary job of this device is to block the ...|positive|\n",
            "+--------------------------------------------------+--------+\n",
            "only showing top 3 rows\n",
            "\n",
            "Balancing data using oversampling...\n",
            "+--------+-----+\n",
            "|   label|count|\n",
            "+--------+-----+\n",
            "| neutral|  772|\n",
            "|positive| 9022|\n",
            "|negative|  467|\n",
            "+--------+-----+\n",
            "\n",
            "Maximum class count: 9022\n",
            "Oversampling class 'neutral' with fraction 11.686528497409327\n",
            "Adjusting sample size for class 'neutral'\n",
            "Keeping class 'positive' as is (majority class)\n",
            "Oversampling class 'negative' with fraction 19.319057815845824\n",
            "Adjusting sample size for class 'negative'\n",
            "Class distribution after balancing:\n",
            "+--------+-----+\n",
            "|   label|count|\n",
            "+--------+-----+\n",
            "| neutral| 9022|\n",
            "|positive| 9022|\n",
            "|negative| 9022|\n",
            "+--------+-----+\n",
            "\n",
            "\n",
            "Statistics on balanced data:\n",
            "+-------+--------------------+--------+\n",
            "|summary|     cleaned_reviews|   label|\n",
            "+-------+--------------------+--------+\n",
            "|  count|               27066|   27066|\n",
            "|   mean|                NULL|    NULL|\n",
            "| stddev|                NULL|    NULL|\n",
            "|    min|                    |negative|\n",
            "|    max|zero issues with ...|positive|\n",
            "+-------+--------------------+--------+\n",
            "\n",
            "Splitting data...\n",
            "Training data: 21835 examples\n",
            "Validation data: 2666 examples\n",
            "Test data: 2565 examples\n",
            "Configuring pipelines...\n",
            "Training Logistic Regression with cross-validation...\n",
            "Training time for Logistic Regression: 0:44:52.633075\n",
            "Validation accuracy for Logistic Regression: 0.9764\n",
            "\n",
            "Class distribution for Logistic Regression:\n",
            "+------------+-----+\n",
            "|indexedLabel|count|\n",
            "+------------+-----+\n",
            "|         1.0|  890|\n",
            "|         2.0|  872|\n",
            "|         0.0|  904|\n",
            "+------------+-----+\n",
            "\n",
            "Training Random Forest with cross-validation...\n",
            "Training time for Random Forest: 0:04:41.023763\n",
            "Validation accuracy for Random Forest: 0.7753\n",
            "\n",
            "Class distribution for Random Forest:\n",
            "+------------+-----+\n",
            "|indexedLabel|count|\n",
            "+------------+-----+\n",
            "|         1.0|  890|\n",
            "|         2.0|  872|\n",
            "|         0.0|  904|\n",
            "+------------+-----+\n",
            "\n",
            "Training Naive Bayes with cross-validation...\n",
            "Training time for Naive Bayes: 0:00:39.321610\n",
            "Validation accuracy for Naive Bayes: 0.9070\n",
            "\n",
            "Class distribution for Naive Bayes:\n",
            "+------------+-----+\n",
            "|indexedLabel|count|\n",
            "+------------+-----+\n",
            "|         1.0|  890|\n",
            "|         2.0|  872|\n",
            "|         0.0|  904|\n",
            "+------------+-----+\n",
            "\n",
            "\n",
            "Best model: Logistic Regression with validation accuracy: 0.9764\n",
            "Metrics for best model:\n",
            "accuracy: 0.9764\n",
            "f1: 0.9762\n",
            "weightedPrecision: 0.9770\n",
            "weightedRecall: 0.9764\n",
            "Test accuracy for Logistic Regression: 0.9700\n",
            "Saving model to: models/logistic_regression_model\n",
            "Model saved successfully: models/logistic_regression_model\n",
            "Test data saved to: data/processed/test_data.json\n",
            "Saving model to: models/tfidf_model\n",
            "Model saved successfully: models/tfidf_model\n",
            "\n",
            "=== Training completed successfully with best model: Logistic Regression ===\n",
            "\n",
            "Cleaning up resources...\n",
            "Spark session closed\n",
            "Total execution time: 0:50:38.264226\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "9o_fmiYF0q7h"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}